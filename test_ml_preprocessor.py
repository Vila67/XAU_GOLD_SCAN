import numpy as np
import pandas as pd
from src.preprocessing.ml_preprocessor import MLPreprocessor
from src.data_collection.historical_data import HistoricalDataLoader
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold
from sklearn.decomposition import PCA
from sklearn.decomposition import KernelPCA

def test_ml_preprocessor():
    """Test des fonctionnalit√©s du MLPreprocessor"""
    print("Test du MLPreprocessor")
    print("="*50)
    
    # 1. Charger les donn√©es
    print("\n1. Chargement des donn√©es")
    loader = HistoricalDataLoader()
    data_dict = None  # Initialiser data_dict √† None
    
    try:
        # Solution 1: V√©rifier les colonnes
        data_dict = loader.load_multi_timeframe_data()
    except KeyError as e:
        if str(e) == "'Date'":
            print("‚ö†Ô∏è Erreur de colonne 'Date' - Tentative de correction...")
            
            # Solution 2: Essayer diff√©rents encodages et s√©parateurs
            try:
                data_dict = loader.load_multi_timeframe_data(encoding="latin-1", sep=";")
            except Exception:
                print("‚ö†Ô∏è √âchec avec latin-1 et ; - Tentative avec autres param√®tres...")
                
                # Solution 3: Essayer sans en-t√™te et d√©tecter automatiquement
                try:
                    data_dict = loader.load_multi_timeframe_data(header=None)
                    
                    # V√©rifier si les donn√©es sont charg√©es correctement
                    if '5m' in data_dict:
                        df_5m = data_dict['5m']
                        print("\nColonnes d√©tect√©es dans 5m:")
                        print(df_5m.columns.tolist())
                        
                        # Renommer les colonnes si n√©cessaire
                        if 'Date' not in df_5m.columns:
                            df_5m.columns = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']
                            data_dict['5m'] = df_5m
                            print("‚úÖ Colonnes renomm√©es avec succ√®s")
                    
                except Exception as e:
                    print(f"‚ùå √âchec du chargement: {str(e)}")
                    raise
    except Exception as e:
        print(f"‚ùå Erreur inattendue: {str(e)}")
        raise
    
    # V√©rifier si les donn√©es ont √©t√© charg√©es
    if data_dict is None:
        raise ValueError("Impossible de charger les donn√©es apr√®s toutes les tentatives")
    
    # 2. Cr√©er le preprocessor
    print("\n2. Initialisation du preprocessor")
    preprocessor = MLPreprocessor()
    
    # 3. Test de la pr√©paration des donn√©es
    print("\n3. Test de la pr√©paration des donn√©es")
    sequence_length = 15
    prediction_horizon = 12
    train_size = 0.8
    target_samples = 3000
    
    try:
        def stratified_kfold_split(X_dict, y, train_size=0.8, n_splits=5):
            """Split stratifi√© avec KFold pour une meilleure r√©partition"""
            from sklearn.model_selection import StratifiedKFold
            
            print("\nApplication du split stratifi√© KFold:")
            print("Distribution initiale des classes:")
            for class_label in np.unique(y):
                count = np.sum(y == class_label)
                ratio = count / len(y) * 100
                print(f"Classe {class_label}: {count} √©chantillons ({ratio:.2f}%)")
            
            # Utiliser StratifiedKFold pour une meilleure r√©partition
            skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
            
            # S√©lectionner le meilleur split
            best_split = None
            min_diff = float('inf')
            
            # Cr√©er un array d'indices de la taille de y
            indices = np.arange(len(y))
            
            for train_idx, test_idx in skf.split(indices, y):
                y_train_fold = y[train_idx]
                y_test_fold = y[test_idx]
                
                # Calculer les diff√©rences de distribution
                train_dist = np.array([np.sum(y_train_fold == c) / len(y_train_fold) 
                                     for c in [-1, 0, 1]])
                test_dist = np.array([np.sum(y_test_fold == c) / len(y_test_fold) 
                                     for c in [-1, 0, 1]])
                
                diff = np.max(np.abs(train_dist - test_dist))
                
                if diff < min_diff:
                    min_diff = diff
                    best_split = (train_idx, test_idx)
            
            if best_split is None:
                raise ValueError("Impossible de trouver un split valide")
            
            # Cr√©er les dictionnaires X_train et X_test
            X_train = {}
            X_test = {}
            
            # Diviser les donn√©es pour chaque timeframe
            timeframes = ['5m', '15m', '30m', '1h', '4h', '1d', '1w', '1M']
            for tf in timeframes:
                if tf in X_dict:
                    # V√©rifier que les indices sont valides
                    max_idx = len(X_dict[tf]['X'])
                    train_indices = best_split[0][best_split[0] < max_idx]
                    test_indices = best_split[1][best_split[1] < max_idx]
                    
                    # Assigner les donn√©es avec les indices valides
                    X_train[tf] = X_dict[tf]['X'][train_indices]
                    X_test[tf] = X_dict[tf]['X'][test_indices]
                    
                    print(f"\nTimeframe {tf}:")
                    print(f"Taille totale: {max_idx}")
                    print(f"Train: {len(train_indices)} √©chantillons")
                    print(f"Test: {len(test_indices)} √©chantillons")
            
            return X_train, X_test, y[best_split[0]], y[best_split[1]]
        
        # Pr√©parer les donn√©es avec le nouveau split stratifi√©
        sequences_dict = preprocessor.create_sequences(
            data_dict,
            sequence_length=sequence_length,
            prediction_horizon=prediction_horizon,
            target_samples=target_samples
        )
        
        # Extraire X_combined et y_combined du dictionnaire
        y_combined = np.concatenate([sequences_dict[tf]['y'] for tf in sequences_dict], axis=0)
        
        # Appliquer le split stratifi√© am√©lior√©
        X_train, X_test, y_train, y_test = stratified_kfold_split(
            sequences_dict,
            y_combined,
            train_size=train_size
        )
        
        # V√©rifier la distribution apr√®s split
        print("\nDistribution apr√®s split stratifi√©:")
        for set_name, y_data in [("Train", y_train), ("Test", y_test)]:
            print(f"\n{set_name}:")
            for class_label in np.unique(y_combined):
                count = np.sum(y_data == class_label)
                ratio = count / len(y_data) * 100
                print(f"Classe {class_label}: {count} √©chantillons ({ratio:.2f}%)")
        
        # Appliquer SMOTE si n√©cessaire pour la classe neutre
        if np.sum(y_train == 0) < 40:  # Minimum de 40 √©chantillons neutres
            print("\nApplication de SMOTE pour la classe neutre:")
            try:
                # Pr√©parer les donn√©es pour SMOTE
                # Calculer d'abord le nombre total de features
                total_features = sum(np.prod(X_train[tf].shape[1:]) for tf in X_train)
                X_train_flat = np.zeros((len(y_train), total_features))
                
                # Remplir X_train_flat avec les donn√©es de chaque timeframe
                current_pos = 0
                for tf in X_train:
                    n_features = np.prod(X_train[tf].shape[1:])
                    X_train_flat[:, current_pos:current_pos+n_features] = X_train[tf].reshape(len(y_train), -1)
                    current_pos += n_features
                
                # Calculer le nombre cible d'√©chantillons neutres
                target_neutral = max(40, int(0.05 * len(y_train)))  # Au moins 5% ou 40 √©chantillons
                
                smote = SMOTE(
                    sampling_strategy={0: target_neutral},
                    random_state=42,
                    k_neighbors=min(5, np.sum(y_train == 0) - 1)
                )
                
                X_resampled, y_resampled = smote.fit_resample(X_train_flat, y_train)
                
                # Reconstruire le dictionnaire X_train
                X_train_new = {}
                current_pos = 0
                for tf in X_train:
                    shape = X_train[tf].shape
                    n_features = np.prod(shape[1:])
                    X_train_new[tf] = X_resampled[:, current_pos:current_pos+n_features].reshape(-1, *shape[1:])
                    current_pos += n_features
                
                X_train = X_train_new
                y_train = y_resampled
                
                print("\nDistribution apr√®s SMOTE:")
                for class_label in np.unique(y_train):
                    count = np.sum(y_train == class_label)
                    ratio = count / len(y_train) * 100
                    print(f"Classe {class_label}: {count} √©chantillons ({ratio:.2f}%)")
                
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur lors de l'application de SMOTE: {str(e)}")
        
        # 4. V√©rifier l'√©quilibre des donn√©es de mani√®re plus d√©taill√©e
        print("\n4. V√©rification d√©taill√©e de l'√©quilibre des donn√©es")
        is_balanced = preprocessor.check_data_balance(y_train, y_test)
        
        # Ajouter une analyse plus d√©taill√©e des proportions
        def print_class_distribution(y, set_name):
            unique, counts = np.unique(y, return_counts=True)
            dist = dict(zip(unique, counts))
            total = sum(counts)
            print(f"\nDistribution des classes ({set_name}):")
            for classe, count in dist.items():
                percentage = (count/total) * 100
                print(f"Classe {classe}: {count} ({percentage:.2f}%)")
        
        print_class_distribution(y_train, "Train")
        print_class_distribution(y_test, "Test")
        
        # Calculer le d√©s√©quilibre entre train et test de mani√®re plus robuste
        def get_class_distribution(y):
            unique, counts = np.unique(y, return_counts=True)
            total = len(y)
            dist = {int(class_): count/total for class_, count in zip(unique, counts)}
            return dist
        
        train_dist = get_class_distribution(y_train)
        test_dist = get_class_distribution(y_test)
        
        # S'assurer que toutes les classes sont pr√©sentes dans les deux distributions
        all_classes = sorted(set(train_dist.keys()) | set(test_dist.keys()))
        for class_ in all_classes:
            if class_ not in train_dist:
                train_dist[class_] = 0
            if class_ not in test_dist:
                test_dist[class_] = 0
        
        # Calculer le d√©s√©quilibre maximal
        max_imbalance = max(abs(train_dist[class_] - test_dist[class_]) * 100 
                           for class_ in all_classes)
        
        print(f"\nD√©s√©quilibre maximal entre train et test: {max_imbalance:.2f}%")
        
        # Afficher les diff√©rences par classe
        print("\nDiff√©rences par classe (Test - Train):")
        for class_ in all_classes:
            diff = (test_dist[class_] - train_dist[class_]) * 100
            print(f"Classe {class_}: {diff:+.2f}%")
        
        if max_imbalance > 5:
            print("‚ö†Ô∏è D√©s√©quilibre significatif d√©tect√© (>5%)")
        if is_balanced:
            print("‚úÖ Les donn√©es sont globalement √©quilibr√©es")
        else:
            print("‚ö†Ô∏è Les donn√©es pr√©sentent un d√©s√©quilibre")
        
        # 5. Afficher les informations sur les features
        print("\n5. Information sur les features")
        for tf in sequences_dict:
            print(f"\nTimeframe {tf}:")
            print(f"Train shape: {X_train[tf].shape}")
            print(f"Test shape: {X_test[tf].shape}")
            print(f"Nombre de features: {X_train[tf].shape[-1]}")
            
            # V√©rifier la normalisation
            print(f"Range train: [{X_train[tf].min():.3f}, {X_train[tf].max():.3f}]")
            print(f"Range test: [{X_test[tf].min():.3f}, {X_test[tf].max():.3f}]")
        
        # Modifier la fonction d'analyse des ratios d'alignement
        print("\n6. Analyse des ratios d'alignement")
        target_ratio = 0.20  # ratio cible pour les timeframes longs
        
        def analyze_timeframe_ratios(X_data):
            base_samples = max([data.shape[0] for data in X_data.values()])
            ratios = {}
            long_timeframes = ['1w', '1M']  # Timeframes longs √† surveiller particuli√®rement
            
            print("\nAnalyse g√©n√©rale:")
            for tf, data in X_data.items():
                ratio = data.shape[0] / base_samples
                ratios[tf] = ratio
                status = "‚úÖ" if ratio >= target_ratio else "‚ö†Ô∏è"
                print(f"{status} {tf}: {ratio:.3f} ({data.shape[0]} √©chantillons)")
                
                if ratio < target_ratio:
                    current_factor = 1.0
                    suggested_factor = min(3.0, target_ratio / ratio)  # Limit√© √† 3x max
                    additional_samples = int((suggested_factor - 1) * data.shape[0])
                    
                    if tf in long_timeframes:
                        print(f"   üìä Timeframe long d√©tect√©: {tf}")
                        print(f"   ‚Üí Ratio actuel: {ratio:.3f}")
                        print(f"   ‚Üí Facteur d'upsampling sugg√©r√©: {suggested_factor:.2f}x")
                        print(f"   ‚Üí √âchantillons suppl√©mentaires: {additional_samples}")
                        if suggested_factor > 2.5:
                            print("   ‚ö†Ô∏è Attention: Facteur √©lev√©, consid√©rer une approche progressive")
                    else:
                        print(f"   ‚Üí Suggestion: Augmenter le facteur d'upsampling de {suggested_factor:.2f}x")
            
            # Analyse sp√©cifique des timeframes longs
            print("\nR√©sum√© des timeframes longs:")
            long_tf_stats = {tf: ratios[tf] for tf in long_timeframes if tf in ratios}
            if long_tf_stats:
                avg_long_ratio = sum(long_tf_stats.values()) / len(long_tf_stats)
                print(f"Ratio moyen des timeframes longs: {avg_long_ratio:.3f}")
                if avg_long_ratio < target_ratio:
                    print(f"‚ö†Ô∏è Les timeframes longs sont sous-repr√©sent√©s (cible: {target_ratio})")
                    print("Suggestions d'optimisation:")
                    for tf, ratio in long_tf_stats.items():
                        optimal_factor = min(3.0, target_ratio / ratio)
                        print(f"- {tf}: Utiliser un facteur d'upsampling de {optimal_factor:.2f}x")
            
            return ratios
        
        print("\nRatios d'alignement (Train):")
        train_ratios = analyze_timeframe_ratios(X_train)
        print("\nRatios d'alignement (Test):")
        test_ratios = analyze_timeframe_ratios(X_test)
        
        # Am√©liorer l'analyse de la classe neutre avec focus sur l'absence dans le train set
        print("\n7. Analyse d√©taill√©e de la classe neutre")
        def analyze_neutral_class(y_data, set_name):
            total = len(y_data)
            neutral_samples = np.sum(y_data == 0)
            neutral_ratio = (neutral_samples / total) * 100
            
            print(f"\nClasse neutre ({set_name}):")
            print(f"Nombre d'√©chantillons: {neutral_samples}")
            print(f"Ratio actuel: {neutral_ratio:.2f}%")
            
            if neutral_samples == 0:
                print("‚ö†Ô∏è CRITIQUE: Classe neutre absente!")
                # Calculer le nombre d'√©chantillons n√©cessaires pour atteindre ~5%
                target_samples = int(0.05 * total)
                print(f"‚Üí N√©cessit√© d'ajouter {target_samples} √©chantillons")
                print("‚Üí Suggestions:")
                print("   1. Utiliser SMOTE ou ADASYN pour g√©n√©rer des √©chantillons synth√©tiques")
                print("   2. Transf√©rer des √©chantillons du test set (si disponible)")
                print("   3. R√©√©valuer la strat√©gie de split train/test")
            elif neutral_ratio < 5:
                print("‚ö†Ô∏è Classe neutre sous-repr√©sent√©e")
                target_samples = int(0.05 * total)
                additional_needed = target_samples - neutral_samples
                print(f"‚Üí Ajouter {additional_needed} √©chantillons pour atteindre 5%")
                if set_name == "Test" and neutral_ratio > 0:
                    print("‚Üí Possibilit√© de transf√©rer des √©chantillons vers le train set")
            elif 5 <= neutral_ratio <= 15:
                print("‚úÖ Ratio acceptable (5-15%)")
            else:
                print("‚ö†Ô∏è Classe neutre sur-repr√©sent√©e")
                suggested_reduction = int(neutral_samples - (0.15 * total))
                print(f"‚Üí R√©duire de {suggested_reduction} √©chantillons pour atteindre 15%")
            
            return neutral_ratio, neutral_samples
        
        train_neutral_ratio, train_neutral_samples = analyze_neutral_class(y_train, "Train")
        test_neutral_ratio, test_neutral_samples = analyze_neutral_class(y_test, "Test")
        
        # Analyse sp√©cifique du transfert possible entre test et train
        if train_neutral_samples == 0 and test_neutral_samples > 0:
            print("\nPlan de r√©√©quilibrage sugg√©r√©:")
            safe_transfer = min(97, int(test_neutral_samples * 0.4))  # Limite √† 40% max du test set
            remaining_needed = 97 - safe_transfer
            if remaining_needed > 0:
                print(f"2. G√©n√©rer {remaining_needed} √©chantillons synth√©tiques additionnels")
            print("\n√âtapes de mise en ≈ìuvre:")
            print("1. Utiliser un split stratifi√© pour la division initiale")
            print("2. Appliquer SMOTE/ADASYN apr√®s le split pour les √©chantillons synth√©tiques")
            print("3. Valider la qualit√© des √©chantillons g√©n√©r√©s")
        
        # Calculer la diff√©rence de ratio entre train et test
        neutral_diff = abs(train_neutral_ratio - test_neutral_ratio)
        print(f"\nDiff√©rence de ratio entre train et test: {neutral_diff:.2f}%")
        if neutral_diff > 2:
            print("‚ö†Ô∏è Diff√©rence significative entre train et test")
            print("‚Üí Suggestion: Ajuster les ratios d'oversampling pour maintenir une distribution similaire")
        
        # Ajouter une analyse de la diff√©rence de distribution train/test
        print("\n8. Analyse de la diff√©rence de distribution train/test")
        def analyze_distribution_split(y_train, y_test):
            train_dist = get_class_distribution(y_train)
            test_dist = get_class_distribution(y_test)
            all_classes = sorted(set(train_dist.keys()) | set(test_dist.keys()))
            
            print("\nComparaison d√©taill√©e des distributions:")
            print("Classe  |  Train   |   Test   | Diff√©rence")
            print("--------+----------+----------+-----------")
            
            max_diff_class = None
            max_diff = 0
            
            for class_ in all_classes:
                train_pct = train_dist.get(class_, 0) * 100
                test_pct = test_dist.get(class_, 0) * 100
                diff = test_pct - train_pct
                
                if abs(diff) > abs(max_diff):
                    max_diff = diff
                    max_diff_class = class_
                
                status = "‚ö†Ô∏è" if abs(diff) > 2 else "‚úÖ"
                print(f"{class_:^7} | {train_pct:>6.2f}% | {test_pct:>6.2f}% | {diff:>+7.2f}% {status}")
            
            print(f"\nClasse la plus d√©s√©quilibr√©e: {max_diff_class} (√©cart de {max_diff:+.2f}%)")
            
            # Suggestions d'am√©lioration
            if abs(max_diff) > 2:
                print("\nSuggestions d'am√©lioration:")
                print("1. Ajustement du split train/test:")
                if max_diff > 0:
                    print(f"   ‚Üí Augmenter la proportion de classe {max_diff_class} dans le train set")
                    suggested_adjustment = abs(max_diff) / 2
                    print(f"   ‚Üí Transf√©rer ~{suggested_adjustment:.1f}% des √©chantillons du test vers le train")
                else:
                    print(f"   ‚Üí Augmenter la proportion de classe {max_diff_class} dans le test set")
                    suggested_adjustment = abs(max_diff) / 2
                    print(f"   ‚Üí Transf√©rer ~{suggested_adjustment:.1f}% des √©chantillons du train vers le test")
                
                print("\n2. Strat√©gies de r√©√©chantillonnage:")
                print("   ‚Üí Utiliser un split stratifi√© pour maintenir les proportions")
                print("   ‚Üí Appliquer l'oversampling apr√®s le split pour √©viter les fuites")
                
                # Calcul du ratio de r√©√©chantillonnage optimal
                target_ratio = (train_dist.get(max_diff_class, 0) + test_dist.get(max_diff_class, 0)) / 2
                print(f"\n3. Ratio cible sugg√©r√© pour la classe {max_diff_class}: {target_ratio*100:.2f}%")
            
            return max_diff
        
        max_distribution_diff = analyze_distribution_split(y_train, y_test)
        
        # Ajouter l'analyse d√©taill√©e de l'asym√©trie train/test
        print("\n9. Analyse d√©taill√©e de l'asym√©trie train/test")
        def analyze_class_asymmetry(y_train, y_test):
            print("\nAnalyse de l'asym√©trie par classe:")
            print("="*50)
            
            # Calculer les distributions
            train_dist = get_class_distribution(y_train)
            test_dist = get_class_distribution(y_test)
            all_classes = sorted(set(train_dist.keys()) | set(test_dist.keys()))
            
            class_labels = {
                -1: "baisse",
                0: "neutre",
                1: "hausse"
            }
            
            # Analyser chaque classe
            asymmetry_stats = {}
            for class_ in all_classes:
                train_pct = train_dist.get(class_, 0) * 100
                test_pct = test_dist.get(class_, 0) * 100
                diff = test_pct - train_pct
                
                label = class_labels.get(class_, str(class_))
                print(f"\nClasse {class_} ({label}):")
                print(f"Train: {train_pct:.2f}% | Test: {test_pct:.2f}% | √âcart: {diff:+.2f}%")
                
                # Suggestions sp√©cifiques par classe
                if abs(diff) > 2:
                    print("‚ö†Ô∏è Asym√©trie significative d√©tect√©e")
                    if diff > 0:
                        print(f"‚Üí La classe est sur-repr√©sent√©e dans le test set ({diff:+.2f}%)")
                        target_adjustment = abs(diff) / 2
                        print(f"‚Üí Suggestion: Transf√©rer {target_adjustment:.1f}% vers le train set")
                    else:
                        print(f"‚Üí La classe est sous-repr√©sent√©e dans le test set ({diff:+.2f}%)")
                        target_adjustment = abs(diff) / 2
                        print(f"‚Üí Suggestion: Augmenter de {target_adjustment:.1f}% dans le test set")
                else:
                    print("‚úÖ Distribution relativement √©quilibr√©e")
                
                asymmetry_stats[class_] = diff
            
            # Suggestions globales
            print("\nSuggestions d'am√©lioration globales:")
            print("="*50)
            print("1. Strat√©gie de split:")
            print("   ‚Üí Impl√©menter un StratifiedKFold pour le split train/test")
            print("   ‚Üí Utiliser un random_state fixe pour la reproductibilit√©")
            
            print("\n2. Strat√©gie d'oversampling:")
            print("   ‚Üí Appliquer l'oversampling s√©par√©ment sur train et test")
            print("   ‚Üí Ajuster les ratios d'oversampling par classe:")
            for class_ in all_classes:
                diff = asymmetry_stats[class_]
                if abs(diff) > 1:
                    label = class_labels.get(class_, str(class_))
                    if diff > 0:
                        print(f"     ‚Ä¢ Classe {class_} ({label}): Augmenter de {abs(diff/2):.1f}% dans le train")
                    else:
                        print(f"     ‚Ä¢ Classe {class_} ({label}): Augmenter de {abs(diff/2):.1f}% dans le test")
            
            return asymmetry_stats
        
        asymmetry_stats = analyze_class_asymmetry(y_train, y_test)
        
        # Ajouter l'analyse du bruit et de l'upsampling contr√¥l√©
        print("\n10. Analyse du bruit et upsampling contr√¥l√©")
        def analyze_noise_upsampling(X_data, timeframes=['1M', '1w', '1d']):
            print("\nAnalyse du bruit et suggestions d'upsampling:")
            print("="*50)
            
            base_samples = max([data.shape[0] for data in X_data.values()])
            noise_configs = {}
            
            # Param√®tres globaux plus conservateurs
            max_global_factor = 2.0  # Limite globale √† 2x
            
            for tf in timeframes:
                if tf not in X_data:
                    continue
                    
                data = X_data[tf]
                current_samples = data.shape[0]
                ratio = current_samples / base_samples
                
                print(f"\nTimeframe {tf}:")
                print(f"√âchantillons actuels: {current_samples}")
                print(f"Ratio actuel: {ratio:.3f}")
                
                # Calculer le facteur d'upsampling optimal avec nouvelle limite
                target_ratio = min(0.20, ratio * 2)  # Maximum 2x ou 20%
                upsampling_factor = min(max_global_factor, target_ratio / ratio)
                
                # Configuration du bruit plus conservative
                if tf == '1M':
                    noise_scale = 0.0005  # R√©duit de moiti√©
                    max_factor = 1.5      # Plus conservateur
                elif tf == '1w':
                    noise_scale = 0.001   # R√©duit de moiti√©
                    max_factor = 1.75     # Plus conservateur
                else:
                    noise_scale = 0.002   # R√©duit de plus de moiti√©
                    max_factor = 2.0      # Limite standard
                
                # Appliquer les limites plus strictes
                upsampling_factor = min(upsampling_factor, max_factor)
                target_samples = int(current_samples * upsampling_factor)
                
                noise_configs[tf] = {
                    'factor': upsampling_factor,
                    'noise_scale': noise_scale,
                    'target_samples': target_samples
                }
                
                print("\nSuggestions d'upsampling (conservatrices):")
                print(f"‚Üí Facteur sugg√©r√©: {upsampling_factor:.2f}x")
                print(f"‚Üí √âchantillons cibles: {target_samples}")
                print(f"‚Üí √âchelle de bruit r√©duite: {noise_scale}")
                
                if upsampling_factor > 1.0:
                    additional = target_samples - current_samples
                    print(f"‚Üí √âchantillons √† g√©n√©rer: {additional}")
                    print("\nRecommandations de bruit (conservatrices):")
                    print(f"‚Ä¢ Utiliser une distribution normale r√©duite: N(0, {noise_scale})")
                    print("‚Ä¢ Appliquer le bruit progressivement")
                    print("‚Ä¢ Valider chaque batch d'√©chantillons g√©n√©r√©s")
                    
                    if upsampling_factor >= 1.5:
                        print("\n‚ö†Ô∏è Attention: Facteur d'upsampling significatif")
                        print("‚Üí Valider avec cross-validation")
                        print("‚Üí Consid√©rer une approche progressive:")
                        print(f"   1. Commencer avec {upsampling_factor/2:.2f}x")
                        print("   2. √âvaluer les performances")
                        print("   3. Augmenter si n√©cessaire")
                
                # Ajouter des m√©triques de qualit√©
                if 'original_std' in locals():
                    quality_ratio = noise_scale / original_std
                    print(f"\nRatio bruit/variance: {quality_ratio:.3f}")
                    if quality_ratio > 0.1:
                        print("‚ö†Ô∏è Niveau de bruit potentiellement √©lev√©")
            
            return noise_configs
        
        print("\nAnalyse du train set:")
        train_noise_configs = analyze_noise_upsampling(X_train)
        print("\nAnalyse du test set:")
        test_noise_configs = analyze_noise_upsampling(X_test)
        
        # Ajouter l'impl√©mentation SMOTE pour la classe neutre
        print("\n11. R√©√©quilibrage de la classe neutre avec SMOTE")
        def apply_smote_resampling(X_train, y_train, X_test, y_test):
            print("\nApplication de SMOTE pour la classe neutre:")
            print("="*50)
            
            # Analyser la situation actuelle
            train_neutral = np.sum(y_train == 0)
            test_neutral = np.sum(y_test == 0)
            test_neutral_ratio = test_neutral / len(y_test)
            
            print(f"\n√âtat initial:")
            print(f"‚Ä¢ Train: {train_neutral} √©chantillons neutres")
            print(f"‚Ä¢ Test: {test_neutral} √©chantillons neutres ({test_neutral_ratio*100:.2f}%)")
            
            if train_neutral == 0:
                # Calculer le nombre d'√©chantillons √† transf√©rer
                safe_transfer = min(16, int(test_neutral * 0.4))
                remaining_needed = 81
                
                print(f"\nPlan de r√©√©quilibrage:")
                print(f"1. Transfert test ‚Üí train: {safe_transfer} √©chantillons")
                print(f"2. G√©n√©ration SMOTE: {remaining_needed} √©chantillons")
                
                # Pr√©parer les indices pour le transfert
                neutral_indices = np.where(y_test == 0)[0]
                transfer_indices = np.random.choice(neutral_indices, safe_transfer, replace=False)
                
                # Effectuer le transfert pour chaque timeframe
                X_train_new = {}
                X_test_new = {}
                
                print("\nTransfert des √©chantillons test ‚Üí train...")
                for tf in X_train.keys():
                    # Transf√©rer les √©chantillons du test vers le train
                    X_train_new[tf] = np.concatenate([X_train[tf], X_test[tf][transfer_indices]])
                    
                    # Mettre √† jour le test set
                    mask = np.ones(len(y_test), dtype=bool)
                    mask[transfer_indices] = False
                    X_test_new[tf] = X_test[tf][mask]
                
                # Mettre √† jour les labels
                y_train_new = np.concatenate([y_train, y_test[transfer_indices]])
                y_test_new = y_test[mask]
                
                # Appliquer SMOTE
                print("\nApplication de SMOTE...")
                target_samples = train_neutral + safe_transfer + remaining_needed
                sampling_strategy = {0: target_samples}
                
                try:
                    # Pr√©parer les donn√©es pour SMOTE (combiner tous les timeframes)
                    X_train_combined = np.concatenate([X_train_new[tf].reshape(X_train_new[tf].shape[0], -1) 
                                                     for tf in X_train_new.keys()], axis=1)
                    
                    smote = SMOTE(sampling_strategy=sampling_strategy, random_state=42)
                    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_combined, y_train_new)
                    
                    # R√©partir les donn√©es resamplees dans les timeframes
                    feature_start = 0
                    X_train_final = {}
                    for tf in X_train_new.keys():
                        original_shape = X_train_new[tf].shape
                        feature_count = np.prod(original_shape[1:])  # Multiplier toutes les dimensions sauf la premi√®re
                        features = X_train_resampled[:, feature_start:feature_start + feature_count]
                        X_train_final[tf] = features.reshape(features.shape[0], *original_shape[1:])
                        feature_start += feature_count
                    
                    # V√©rifier les r√©sultats
                    final_train_neutral = np.sum(y_train_resampled == 0)
                    final_test_neutral = np.sum(y_test_new == 0)
                    
                    print("\nR√©sultats du r√©√©quilibrage:")
                    print(f"Train: {final_train_neutral} √©chantillons neutres")
                    print(f"Test: {final_test_neutral} √©chantillons neutres")
                    
                    return X_train_final, y_train_resampled, X_test_new, y_test_new
                    
                except Exception as e:
                    print(f"‚ùå Erreur lors de l'application de SMOTE: {str(e)}")
                    return X_train, y_train, X_test, y_test
            else:
                print("‚úÖ Classe neutre d√©j√† pr√©sente dans le train set")
                return X_train, y_train, X_test, y_test
        
        # Appliquer le r√©√©quilibrage
        X_train, y_train, X_test, y_test = apply_smote_resampling(X_train, y_train, X_test, y_test)
        
        # Ajouter le r√©√©quilibrage de la classe hausse (1)
        print("\n12. R√©√©quilibrage de la classe hausse (1)")
        def balance_positive_class(X_train, y_train, X_test, y_test):
            """√âquilibre la classe +1 entre train et test"""
            print("\n√âquilibrage de la classe +1:")
            print("="*50)
            
            # Calculer les ratios actuels
            train_pos = np.sum(y_train == 1) / len(y_train) * 100
            test_pos = np.sum(y_test == 1) / len(y_test) * 100
            diff = train_pos - test_pos
            
            print(f"Distribution initiale classe +1:")
            print(f"Train: {train_pos:.2f}%")
            print(f"Test: {test_pos:.2f}%")
            print(f"Diff√©rence: {diff:.2f}%")
            
            if diff > 2:  # Si d√©s√©quilibre significatif
                # Calculer le nombre d'√©chantillons √† transf√©rer
                n_samples_to_transfer = int((diff/2) * len(y_train) / 100)
                print(f"\nTransfert de {n_samples_to_transfer} √©chantillons de Train vers Test")
                
                # S√©lectionner les indices √† transf√©rer
                pos_indices = np.where(y_train == 1)[0]
                transfer_indices = np.random.choice(pos_indices, n_samples_to_transfer, replace=False)
                
                # Effectuer le transfert
                X_train_new = {}
                X_test_new = {}
                
                for tf in X_train.keys():
                    # Ajouter au test
                    X_test_new[tf] = np.concatenate([X_test[tf], X_train[tf][transfer_indices]])
                    # Retirer du train
                    mask = np.ones(len(y_train), dtype=bool)
                    mask[transfer_indices] = False
                    X_train_new[tf] = X_train[tf][mask]
                
                y_test_new = np.concatenate([y_test, y_train[transfer_indices]])
                y_train_new = y_train[mask]
                
                # V√©rifier les nouveaux ratios
                new_train_pos = np.sum(y_train_new == 1) / len(y_train_new) * 100
                new_test_pos = np.sum(y_test_new == 1) / len(y_test_new) * 100
                new_diff = new_train_pos - new_test_pos
                
                print(f"\nNouvelle distribution classe +1:")
                print(f"Train: {new_train_pos:.2f}%")
                print(f"Test: {new_test_pos:.2f}%")
                print(f"Diff√©rence: {new_diff:.2f}%")
                
                return X_train_new, y_train_new, X_test_new, y_test_new
            
            return X_train, y_train, X_test, y_test
        
        # Appliquer le r√©√©quilibrage de la classe hausse
        X_train, y_train, X_test, y_test = balance_positive_class(X_train, y_train, X_test, y_test)
        
        # Pr√©parer les donn√©es pour la r√©duction de dimensionnalit√©
        print("\nPr√©paration des donn√©es pour la r√©duction de dimensionnalit√©")
        X_combined = prepare_data_for_reduction(X_train)

        # Optimiser la r√©duction de dimensionnalit√©
        print("\nApplication de la r√©duction de dimensionnalit√©:")
        X_reduced, reducer = optimize_dimensionality_reduction(X_combined)

        # V√©rifier les r√©sultats
        print("\nR√©sultats de la r√©duction:")
        print(f"‚Ä¢ Dimensions initiales: {X_combined.shape}")
        print(f"‚Ä¢ Dimensions r√©duites: {X_reduced.shape}")

        # Calculer les m√©triques de qualit√©
        variances = np.var(X_reduced, axis=0)
        print("\nAnalyse des composantes:")
        print(f"‚Ä¢ Variance minimale: {np.min(variances):.2e}")
        print(f"‚Ä¢ Variance m√©diane: {np.median(variances):.2e}")
        print(f"‚Ä¢ Composantes significatives (var >= 1e-5): {np.sum(variances >= 1e-5)}")
        
        # Ajouter l'√©quilibrage de la classe neutre
        print("\nApplication de l'√©quilibrage de la classe neutre")
        X_train, y_train = balance_neutral_class(X_train, y_train)
        
        # Ajouter l'√©quilibrage des timeframes longs
        print("\nApplication de l'√©quilibrage des timeframes longs")
        sequences_dict = balance_long_timeframes(sequences_dict, min_ratio=0.01)
        
        return X_train, y_train, X_test, y_test
    
    except Exception as e:
        print(f"\n‚ùå Erreur lors du test: {str(e)}")
        raise

def test_feature_generation():
    """Test de la g√©n√©ration des features"""
    print("\nTest de la g√©n√©ration des features")
    print("="*50)
    
    # Charger un √©chantillon de donn√©es
    loader = HistoricalDataLoader()
    data_dict = loader.load_multi_timeframe_data()
    
    # Prendre un timeframe pour le test
    test_tf = '1h'
    test_data = data_dict[test_tf]
    
    preprocessor = MLPreprocessor()
    
    try:
        # Tester la pr√©paration des features
        features_df = preprocessor.prepare_features(test_data)
        
        # G√©rer les valeurs manquantes de mani√®re moderne
        if isinstance(features_df, pd.DataFrame):
            # Remplacer fillna par ffill/bfill pour les DataFrames
            features_df = features_df.ffill().bfill()
        elif isinstance(features_df, dict):
            # Traiter chaque DataFrame dans le dictionnaire
            for key in features_df:
                if isinstance(features_df[key], pd.DataFrame):
                    features_df[key] = features_df[key].ffill().bfill()
        
        print(f"\nFeatures g√©n√©r√©es pour {test_tf}:")
        print(f"Shape: {features_df.shape}")
        print("\nAper√ßu des features:")
        print(features_df.head())
        
        # V√©rifier la normalisation
        print("\nStatistiques des features:")
        if isinstance(features_df, pd.DataFrame):
            # Calculer les statistiques descriptives
            stats = features_df.describe()
            print(f"Minimum: {stats.loc['min'].min():.3f}")
            print(f"Maximum: {stats.loc['max'].max():.3f}")
            print(f"Moyenne: {stats.loc['mean'].mean():.3f}")
            print(f"√âcart-type: {stats.loc['std'].mean():.3f}")
            
            # Afficher des informations suppl√©mentaires
            print("\nDistribution des valeurs:")
            print(f"25%: {stats.loc['25%'].mean():.3f}")
            print(f"50%: {stats.loc['50%'].mean():.3f}")
            print(f"75%: {stats.loc['75%'].mean():.3f}")
            
            # V√©rifier les valeurs manquantes
            null_count = features_df.isnull().sum().sum()
            if null_count > 0:
                print(f"\n‚ö†Ô∏è {null_count} valeurs manquantes d√©tect√©es")
            else:
                print("\n‚úÖ Aucune valeur manquante")
        
        return features_df
        
    except Exception as e:
        print(f"\n‚ùå Erreur lors du test des features: {str(e)}")
        raise

def optimize_dimensionality_reduction(X_train_filtered, method='pca', min_explained_variance=0.90, recursion_count=0):
    """Optimise la r√©duction de dimensionnalit√© avec PCA ou KernelPCA"""
    # Limiter la r√©cursion
    max_recursions = 2
    if recursion_count >= max_recursions:
        print("\n‚ö†Ô∏è Nombre maximum de tentatives atteint")
        print("‚Üí Utilisation de PCA conservative")
        # Fallback sur PCA simple avec param√®tres conservateurs
        n_components = min(50, X_train_filtered.shape[1], X_train_filtered.shape[0])
        pca = PCA(n_components=n_components)
        return pca.fit_transform(X_train_filtered), pca
    
    print(f"\nOptimisation de la r√©duction de dimensionnalit√© (tentative {recursion_count + 1}/{max_recursions}):")
    print(f"M√©thode: {method.upper()}")
    
    try:
        if method == 'pca':
            # Commencer avec une analyse PCA compl√®te
            pca_analyzer = PCA()
            pca_analyzer.fit(X_train_filtered)
            
            # Analyser la courbe de variance expliqu√©e
            cumsum = np.cumsum(pca_analyzer.explained_variance_ratio_)
            
            # Trouver le nombre optimal de composantes
            n_components = np.argmax(cumsum >= min_explained_variance) + 1
            total_components = min(X_train_filtered.shape[1], X_train_filtered.shape[0])
            
            print(f"\nAnalyse de la variance expliqu√©e:")
            print(f"‚Ä¢ Composantes totales possibles: {total_components}")
            print(f"‚Ä¢ Variance expliqu√©e cumul√©e:")
            for threshold in [0.80, 0.85, 0.90, 0.95, 0.99]:
                n = min(np.argmax(cumsum >= threshold) + 1, total_components)
                print(f"  {threshold:.0%}: {n} composantes ({cumsum[n-1]:.2%} de variance)")
            
            # Limiter la r√©duction pour pr√©server l'information
            max_reduction = 0.5  # Ne pas r√©duire de plus de 50%
            min_components = min(int(total_components * max_reduction), total_components)
            n_components = min(max(n_components, min_components), total_components)
            
            print(f"\nParam√®tres retenus:")
            print(f"‚Ä¢ Composantes: {n_components} ({n_components/total_components:.1%} des features)")
            print(f"‚Ä¢ Variance expliqu√©e: {cumsum[n_components-1]:.2%}")
            
            # Appliquer PCA avec les param√®tres optimis√©s
            pca = PCA(n_components=n_components, svd_solver='full')
            X_reduced = pca.fit_transform(X_train_filtered)
            
            # V√©rifier la qualit√© de la r√©duction
            reconstruction_error = calculate_reconstruction_error(
                X_train_filtered,
                pca.inverse_transform(X_reduced)
            )
            
            if reconstruction_error > 0.1 and recursion_count < max_recursions:
                print("\n‚ö†Ô∏è Erreur de reconstruction trop √©lev√©e")
                print("‚Üí Tentative avec KernelPCA")
                return optimize_dimensionality_reduction(
                    X_train_filtered, 
                    method='kpca',
                    min_explained_variance=min_explained_variance,
                    recursion_count=recursion_count + 1
                )
            
            return X_reduced, pca
            
        elif method == 'kpca':
            # Configuration de KernelPCA
            n_components = min(int(X_train_filtered.shape[1] * 0.7), 100)
            kernels = ['rbf', 'poly', 'cosine']
            best_config = None
            best_score = -np.inf
            
            print("\nRecherche de la meilleure configuration KernelPCA:")
            for kernel in kernels:
                for gamma in [0.01, 0.1, 1.0]:
                    try:
                        kpca = KernelPCA(
                            n_components=n_components,
                            kernel=kernel,
                            gamma=gamma,
                            n_jobs=-1
                        )
                        X_kpca = kpca.fit_transform(X_train_filtered)
                        
                        # √âvaluer la qualit√©
                        reconstruction = kpca.inverse_transform(X_kpca)
                        error = calculate_reconstruction_error(X_train_filtered, reconstruction)
                        variances = np.var(X_kpca, axis=0)
                        significant = np.sum(variances >= 1e-5)
                        
                        # Score composite
                        quality_score = (
                            (1 - error) * 0.4 +  # Faible erreur
                            (significant/n_components) * 0.4 +  # Composantes significatives
                            (np.mean(variances)/np.std(variances)) * 0.2  # Bon ratio signal/bruit
                        )
                        
                        print(f"\nKernel: {kernel}, Gamma: {gamma}")
                        print(f"‚Ä¢ Erreur: {error:.4f}")
                        print(f"‚Ä¢ Composantes significatives: {significant}")
                        print(f"‚Ä¢ Score: {quality_score:.4f}")
                        
                        if quality_score > best_score:
                            best_score = quality_score
                            best_config = (kernel, gamma, X_kpca, kpca)
                            
                    except Exception as e:
                        print(f"‚ö†Ô∏è √âchec avec {kernel}, gamma={gamma}: {str(e)}")
                        continue
            
            if best_config is None:
                print("\n‚ö†Ô∏è √âchec de KernelPCA")
                print("‚Üí Retour √† PCA avec param√®tres conservateurs")
                return optimize_dimensionality_reduction(
                    X_train_filtered,
                    method='pca',
                    min_explained_variance=0.95,
                    recursion_count=recursion_count + 1
                )
            
            kernel, gamma, X_reduced, kpca = best_config
            print(f"\n‚úÖ Meilleure configuration:")
            print(f"‚Ä¢ Kernel: {kernel}")
            print(f"‚Ä¢ Gamma: {gamma}")
            print(f"‚Ä¢ Score: {best_score:.4f}")
            
            return X_reduced, kpca
            
    except Exception as e:
        print(f"\n‚ö†Ô∏è Erreur lors de la r√©duction ({method}): {str(e)}")
        if recursion_count < max_recursions:
            print("‚Üí Tentative avec m√©thode alternative")
            new_method = 'kpca' if method == 'pca' else 'pca'
            return optimize_dimensionality_reduction(
                X_train_filtered,
                method=new_method,
                min_explained_variance=min_explained_variance,
                recursion_count=recursion_count + 1
            )
        else:
            print("‚Üí Utilisation de PCA basique")
            n_components = min(50, X_train_filtered.shape[1], X_train_filtered.shape[0])
            pca = PCA(n_components=n_components)
            return pca.fit_transform(X_train_filtered), pca

def calculate_reconstruction_error(original, reconstructed):
    """Calcule l'erreur de reconstruction avec epsilon pour √©viter la division par z√©ro"""
    epsilon = 1e-8
    error = np.mean(np.abs((original - reconstructed) / (original + epsilon)))
    return error

def apply_progressive_upsampling(X_dict, timeframe, current_ratio, target_ratio=0.20, max_factor=2.0):
    """Applique un upsampling progressif pour les timeframes longs"""
    print(f"\nUpsampling progressif pour {timeframe}:")
    
    # Calculer le facteur initial (2.0x maximum)
    initial_factor = min(2.0, target_ratio / current_ratio)
    data = X_dict[timeframe]['X']  # Acc√©der aux donn√©es X directement
    
    # Premi√®re phase d'upsampling avec contr√¥le adaptatif du bruit
    n_samples = len(data)
    target_samples = int(n_samples * initial_factor)
    
    # Calculer l'√©chelle de bruit adaptative bas√©e sur la variance des donn√©es
    data_std = np.std(data, axis=0)
    base_noise_scale = {
        '1d': 0.0001,
        '1w': 0.0002,
        '1M': 0.0003
    }.get(timeframe, 0.0001)
    
    # Ajuster l'√©chelle du bruit en fonction de la variance des donn√©es
    noise_scale = base_noise_scale * np.mean(data_std)
    
    print(f"Phase d'upsampling:")
    print(f"‚Ä¢ √âchantillons initiaux: {n_samples}")
    print(f"‚Ä¢ Cible: {target_samples}")
    print(f"‚Ä¢ Facteur: {initial_factor:.2f}x")
    print(f"‚Ä¢ √âchelle de bruit adaptative: {noise_scale:.6f}")
    
    # G√©n√©rer les nouveaux √©chantillons avec bruit contr√¥l√©
    if target_samples > n_samples:
        # Utiliser SMOTE pour les premiers √©chantillons
        try:
            print("\nApplication de SMOTE pour la g√©n√©ration initiale...")
            smote = SMOTE(
                sampling_strategy='auto',
                k_neighbors=min(5, n_samples-1),
                random_state=42
            )
            
            # Aplatir les donn√©es pour SMOTE
            X_flat = data.reshape(n_samples, -1)
            y_dummy = np.zeros(n_samples)  # Labels fictifs pour SMOTE
            
            X_resampled, _ = smote.fit_resample(X_flat, y_dummy)
            new_samples = X_resampled[n_samples:target_samples].reshape(-1, *data.shape[1:])
            
            # Ajouter un bruit minimal pour √©viter les duplicats exacts
            noise = np.random.normal(0, noise_scale/10, size=new_samples.shape)
            new_samples += noise
            
        except Exception as e:
            print(f"‚ö†Ô∏è SMOTE a √©chou√© ({str(e)}), utilisation de l'approche par bruit...")
            # S√©lection al√©atoire avec bruit
            indices = np.random.choice(n_samples, target_samples - n_samples)
            new_samples = data[indices].copy()
            
            # Appliquer un bruit gaussien avec d√©croissance
            for i, sample in enumerate(new_samples):
                decay = 1 - (i / len(new_samples))  # D√©croissance lin√©aire du bruit
                noise = np.random.normal(0, noise_scale * decay, size=sample.shape)
                new_samples[i] += noise
        
        # Combiner avec les donn√©es originales
        upsampled_data = np.concatenate([data, new_samples])
        
        # V√©rifier la qualit√© des donn√©es g√©n√©r√©es
        original_std = np.std(data, axis=0)
        new_std = np.std(upsampled_data, axis=0)
        std_ratio = np.mean(new_std / original_std)
        
        print("\nV√©rification de la qualit√©:")
        print(f"‚Ä¢ Ratio de d√©viation standard: {std_ratio:.3f}")
        if std_ratio > 1.1:
            print("‚ö†Ô∏è Variance l√©g√®rement augment√©e - Ajustement du bruit")
            # R√©duire le bruit si n√©cessaire
            upsampled_data[n_samples:] = (upsampled_data[n_samples:] + data.mean(axis=0)) / 2
    else:
        upsampled_data = data
    
    final_ratio = len(upsampled_data) / len(X_dict['5m']['X'])
    print(f"\nR√©sultat final:")
    print(f"‚Ä¢ Ratio initial: {current_ratio:.3f}")
    print(f"‚Ä¢ Ratio final: {final_ratio:.3f}")
    print(f"‚Ä¢ Facteur effectif: {len(upsampled_data)/n_samples:.2f}x")
    
    return upsampled_data

def prepare_data_for_reduction(X_train_dict):
    """Pr√©pare les donn√©es pour la r√©duction de dimensionnalit√©"""
    # Calculer le nombre total de features
    total_features = sum(np.prod(X_train_dict[tf].shape[1:]) for tf in X_train_dict)
    min_samples = min(X_train_dict[tf].shape[0] for tf in X_train_dict)
    
    print(f"\nPr√©paration des donn√©es:")
    print(f"‚Ä¢ Total features: {total_features}")
    print(f"‚Ä¢ √âchantillons: {min_samples}")
    
    # Cr√©er le tableau combin√©
    X_combined = np.zeros((min_samples, total_features))
    current_pos = 0
    
    for tf in X_train_dict:
        # Prendre les min_samples premiers √©chantillons
        data = X_train_dict[tf][:min_samples]
        n_features = np.prod(data.shape[1:])
        
        # Aplatir et ajouter au tableau combin√©
        X_combined[:, current_pos:current_pos+n_features] = data.reshape(min_samples, -1)
        current_pos += n_features
        
        print(f"‚Ä¢ {tf}: {n_features} features ajout√©es")
    
    return X_combined

def balance_neutral_class(X_train, y_train, target_ratio=0.05):
    """√âquilibre la classe neutre pour atteindre le ratio cible"""
    print("\n√âquilibrage de la classe neutre:")
    print("="*50)
    
    # Analyser la distribution initiale
    n_samples = len(y_train)
    n_neutral = np.sum(y_train == 0)
    current_ratio = n_neutral / n_samples
    
    print(f"Distribution initiale:")
    print(f"‚Ä¢ Total √©chantillons: {n_samples}")
    print(f"‚Ä¢ √âchantillons neutres: {n_neutral}")
    print(f"‚Ä¢ Ratio actuel: {current_ratio:.3%}")
    print(f"‚Ä¢ Ratio cible: {target_ratio:.3%}")
    
    if current_ratio >= target_ratio:
        print("‚úÖ Classe neutre suffisamment repr√©sent√©e")
        return X_train, y_train
    
    # Calculer le nombre d'√©chantillons √† ajouter
    target_neutral = int(n_samples * target_ratio)
    samples_to_add = target_neutral - n_neutral
    
    print(f"\nG√©n√©ration de {samples_to_add} nouveaux √©chantillons neutres")
    
    # Approche par interpolation directe
    neutral_indices = np.where(y_train == 0)[0]
    X_train_new = {tf: X_train[tf].copy() for tf in X_train}
    
    # V√©rifier la taille minimale pour chaque timeframe
    min_samples = min(X_train[tf].shape[0] for tf in X_train)
    valid_neutral_indices = neutral_indices[neutral_indices < min_samples]
    
    if len(valid_neutral_indices) < 2:
        print("‚ö†Ô∏è Pas assez d'√©chantillons neutres valides pour l'interpolation")
        return X_train, y_train
    
    print(f"‚Ä¢ √âchantillons neutres valides: {len(valid_neutral_indices)}")
    
    # G√©n√©rer les nouveaux √©chantillons par lots
    batch_size = 1000
    remaining = samples_to_add
    
    while remaining > 0:
        current_batch = min(batch_size, remaining)
        
        # S√©lectionner des paires d'indices al√©atoires
        idx1 = np.random.choice(valid_neutral_indices, current_batch)
        idx2 = np.random.choice(valid_neutral_indices, current_batch)
        
        # G√©n√©rer des coefficients d'interpolation
        alphas = np.random.uniform(0.2, 0.8, current_batch)
        
        for tf in X_train:
            # Cr√©er le batch de nouveaux √©chantillons
            new_samples = np.zeros((current_batch,) + X_train[tf].shape[1:])
            
            for i in range(current_batch):
                # Interpolation avec bruit
                new_samples[i] = (
                    alphas[i] * X_train[tf][idx1[i]] + 
                    (1 - alphas[i]) * X_train[tf][idx2[i]] +
                    np.random.normal(0, 0.0001, X_train[tf].shape[1:])
                )
            
            # Ajouter les nouveaux √©chantillons
            X_train_new[tf] = np.concatenate([X_train_new[tf], new_samples])
        
        # Mettre √† jour le compteur
        remaining -= current_batch
        
        # Afficher la progression
        if remaining % 5000 == 0:
            print(f"‚Ä¢ Progression: {samples_to_add - remaining}/{samples_to_add}")
    
    # Ajouter les labels
    y_train_new = np.concatenate([y_train, np.zeros(samples_to_add)])
    
    # V√©rifier la distribution finale
    final_neutral = np.sum(y_train_new == 0)
    final_ratio = final_neutral / len(y_train_new)
    
    print("\nR√©sultats finaux:")
    print(f"‚Ä¢ √âchantillons neutres: {final_neutral}")
    print(f"‚Ä¢ Ratio final: {final_ratio:.3%}")
    print(f"‚Ä¢ Augmentation: {final_neutral/n_neutral:.1f}x")
    
    return X_train_new, y_train_new

def balance_long_timeframes(sequences_dict, min_ratio=0.01):
    """√âquilibre les timeframes longs pour assurer une repr√©sentation minimale"""
    print("\n√âquilibrage des timeframes longs:")
    print("="*50)
    
    # Calculer la taille de r√©f√©rence (5m)
    base_size = len(sequences_dict['5m']['X'])
    print(f"Taille de r√©f√©rence (5m): {base_size} √©chantillons")
    
    # Timeframes √† √©quilibrer
    long_timeframes = ['1d', '1w', '1M']
    balanced_dict = sequences_dict.copy()
    
    for tf in long_timeframes:
        if tf not in sequences_dict:
            continue
            
        current_size = len(sequences_dict[tf]['X'])
        current_ratio = current_size / base_size
        
        print(f"\nTimeframe {tf}:")
        print(f"‚Ä¢ Taille actuelle: {current_size}")
        print(f"‚Ä¢ Ratio actuel: {current_ratio:.3%}")
        
        if current_ratio < min_ratio:
            target_size = int(base_size * min_ratio)
            samples_to_add = target_size - current_size
            
            print(f"‚Ä¢ Cible: {target_size} ({min_ratio:.1%} de 5m)")
            print(f"‚Ä¢ √âchantillons √† ajouter: {samples_to_add}")
            
            try:
                # Approche hybride SMOTE + interpolation
                X = sequences_dict[tf]['X']
                y = sequences_dict[tf]['y']
                
                # Phase 1: SMOTE pour doubler les donn√©es
                smote_size = min(current_size * 2, target_size)
                if smote_size > current_size:
                    print("\nPhase 1: SMOTE")
                    smote = SMOTE(
                        sampling_strategy='all',
                        k_neighbors=min(5, current_size-1),
                        random_state=42
                    )
                    
                    X_flat = X.reshape(current_size, -1)
                    X_smote, y_smote = smote.fit_resample(X_flat, y)
                    X_new = X_smote.reshape(-1, *X.shape[1:])
                    
                    print(f"‚Ä¢ G√©n√©r√©s: {len(X_new) - current_size}")
                else:
                    X_new = X
                    y_smote = y
                
                # Phase 2: Interpolation si n√©cessaire
                remaining = target_size - len(X_new)
                if remaining > 0:
                    print("\nPhase 2: Interpolation")
                    
                    # G√©n√©rer par lots
                    batch_size = 1000
                    while remaining > 0:
                        current_batch = min(batch_size, remaining)
                        
                        # S√©lectionner des paires al√©atoires
                        idx1 = np.random.choice(len(X_new), current_batch)
                        idx2 = np.random.choice(len(X_new), current_batch)
                        weights = np.random.uniform(0.3, 0.7, current_batch)
                        
                        # Interpolation avec bruit adaptatif
                        batch_samples = np.zeros((current_batch,) + X.shape[1:])
                        for i in range(current_batch):
                            sample = (
                                weights[i] * X_new[idx1[i]] +
                                (1 - weights[i]) * X_new[idx2[i]]
                            )
                            
                            # Bruit adaptatif bas√© sur la variance locale
                            local_std = np.std([X_new[idx1[i]], X_new[idx2[i]]], axis=0)
                            noise_scale = 0.1 * local_std
                            noise = np.random.normal(0, noise_scale)
                            
                            batch_samples[i] = sample + noise
                        
                        X_new = np.concatenate([X_new, batch_samples])
                        y_smote = np.concatenate([y_smote, y[idx1]])
                        remaining -= current_batch
                        
                        print(f"‚Ä¢ Progression: {target_size - remaining}/{target_size}")
                
                # V√©rifier la qualit√©
                print("\nV√©rification de la qualit√©:")
                orig_std = np.std(X, axis=0)
                new_std = np.std(X_new, axis=0)
                std_ratio = np.mean(new_std / orig_std)
                print(f"‚Ä¢ Ratio de d√©viation standard: {std_ratio:.3f}")
                
                # Ajuster si n√©cessaire
                if std_ratio > 1.2:
                    print("‚ö†Ô∏è Variance trop √©lev√©e - Application de r√©gularisation")
                    X_new[current_size:] = (
                        X_new[current_size:] + 
                        np.mean(X, axis=0)
                    ) / 2
                
                # Mettre √† jour le dictionnaire
                balanced_dict[tf] = {
                    'X': X_new,
                    'y': y_smote
                }
                
                final_ratio = len(X_new) / base_size
                print(f"\nR√©sultat final pour {tf}:")
                print(f"‚Ä¢ Taille finale: {len(X_new)}")
                print(f"‚Ä¢ Ratio final: {final_ratio:.3%}")
                
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur lors de l'√©quilibrage de {tf}: {str(e)}")
                print("‚Üí Conservation des donn√©es originales")
                continue
    
    return balanced_dict

def test_class_balance():
    """Test l'√©quilibrage des classes dans le pr√©processeur"""
    print("Test de l'√©quilibrage des classes")
    print("="*50)
    
    # 1. Charger les donn√©es
    print("\n1. Chargement des donn√©es...")
    loader = HistoricalDataLoader()
    data_dict = loader.load_multi_timeframe_data()
    
    # 2. Cr√©er le pr√©processeur
    print("\n2. Initialisation du pr√©processeur...")
    preprocessor = MLPreprocessor()
    
    # 3. Pr√©parer les s√©quences avec √©quilibrage des classes
    print("\n3. Pr√©paration des s√©quences...")
    sequences = preprocessor.prepare_sequences(
        data_dict,
        sequence_length=15,
        prediction_horizon=12
    )
    
    # 4. V√©rifier la distribution des classes pour chaque timeframe
    print("\n4. V√©rification des distributions...")
    for tf in sequences.keys():
        print(f"\nTimeframe: {tf}")
        print("-" * 30)
        
        # Distribution globale
        y = sequences[tf]['y']
        unique, counts = np.unique(y, return_counts=True)
        distribution = dict(zip(unique, counts))
        total = sum(counts)
        
        print("Distribution globale:")
        for label in sorted(distribution.keys()):
            count = distribution[label]
            percentage = (count / total) * 100
            print(f"Classe {label}: {count} ({percentage:.1f}%)")
        
        # Distribution train/test
        for split in ['train', 'test']:
            y_split = sequences[tf][f'y_{split}']
            unique, counts = np.unique(y_split, return_counts=True)
            distribution = dict(zip(unique, counts))
            total = sum(counts)
            
            print(f"\nDistribution {split}:")
            for label in sorted(distribution.keys()):
                count = distribution[label]
                percentage = (count / total) * 100
                print(f"Classe {label}: {count} ({percentage:.1f}%)")

if __name__ == "__main__":
    try:
        # Tester le preprocessor
        X_train, y_train, X_test, y_test = test_ml_preprocessor()
        
        # Tester la g√©n√©ration de features
        features = test_feature_generation()
        
        # Tester l'√©quilibrage des classes
        test_class_balance()
        
        print("\n‚úÖ Tests termin√©s avec succ√®s!")
        
    except Exception as e:
        print(f"\n‚ùå Tests √©chou√©s: {str(e)}")
        raise 